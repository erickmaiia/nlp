{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zg4muK1ei_KA"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk import FreqDist, WordNetLemmatizer, word_tokenize, PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dms-Ago5i_KB",
        "outputId": "c2c3006b-a188-41ac-e70a-a843d3b3d5f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nlp = spacy.load(\"en_core_web_sm\") # python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBJYBFgti_KC"
      },
      "source": [
        "1) Calcule os seguintes itens:<br>\n",
        "a) Número total de palavras<br>\n",
        "b) Número total de sentenças<br>\n",
        "c) Número total de palavras não repetidas<br>\n",
        "d) Número total de palavras repetidas<br>\n",
        "e) Média de palavras por sentenças<br>\n",
        "\n",
        "Esse procedimento deve ser feito para os seguintes textos\n",
        "disponíveis na biblioteca NLTK:<br>\n",
        "- shakespeare-caesar.txt,<br>\n",
        "- shakespeare-hamlet.txt,<br>\n",
        "- shakespeare-macbeth.txt<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9p_4VxdYi_KD"
      },
      "outputs": [],
      "source": [
        "sc = \"shakespeare-caesar.txt\"\n",
        "sh = \"shakespeare-hamlet.txt\"\n",
        "sm = \"shakespeare-macbeth.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnRzjN-Ni_KD",
        "outputId": "73b44f9c-d2b6-4b50-9db6-db2a2f1917f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in Caesar:  20802\n",
            "Number of words in Hamlet:  30266\n",
            "Number of words in Macbeth:  18272\n"
          ]
        }
      ],
      "source": [
        "# a)\n",
        "\n",
        "sc_words = gutenberg.words(sc)\n",
        "sh_words = gutenberg.words(sh)\n",
        "sm_words = gutenberg.words(sm)\n",
        "\n",
        "sc_norm_words = [word.lower() for word in sc_words if word.isalpha()]\n",
        "sh_norm_words = [word.lower() for word in sh_words if word.isalpha()]\n",
        "sm_norm_words = [word.lower() for word in sm_words if word.isalpha()]\n",
        "\n",
        "sc_number_of_words = len(sc_norm_words)\n",
        "sh_number_of_words = len(sh_norm_words)\n",
        "sm_number_of_words = len(sm_norm_words)\n",
        "\n",
        "print(\"Number of words in Caesar: \" , sc_number_of_words)\n",
        "print(\"Number of words in Hamlet: \" , sh_number_of_words)\n",
        "print(\"Number of words in Macbeth: \" , sm_number_of_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T9Cz0wLi_KD",
        "outputId": "61674997-ffb9-40cc-c7ae-8ef0823a9f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in Caesar:  2163\n",
            "Number of sentences in Hamlet:  3106\n",
            "Number of sentences in Macbeth:  1907\n"
          ]
        }
      ],
      "source": [
        "# b)\n",
        "\n",
        "sc_sents = gutenberg.sents(sc)\n",
        "sh_sents = gutenberg.sents(sh)\n",
        "sm_sents = gutenberg.sents(sm)\n",
        "\n",
        "print(\"Number of sentences in Caesar: \" , len(sc_sents))\n",
        "print(\"Number of sentences in Hamlet: \"  ,len(sh_sents))\n",
        "print(\"Number of sentences in Macbeth: \" , len(sm_sents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXgr4k7Qi_KD",
        "outputId": "1ac77eba-8d36-401a-e2ec-fe0ce71786b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in Caesar:  3014\n",
            "Number of unique words in Hamlet:  4699\n",
            "Number of unique words in Macbeth:  3447\n"
          ]
        }
      ],
      "source": [
        "# c)\n",
        "\n",
        "sc_not_repeated = set(sc_norm_words)\n",
        "sh_not_repeated = set(sh_norm_words)\n",
        "sm_not_repeated = set(sm_norm_words)\n",
        "\n",
        "print(\"Number of unique words in Caesar: \" , len(sc_not_repeated))\n",
        "print(\"Number of unique words in Hamlet: \" , len(sh_not_repeated))\n",
        "print(\"Number of unique words in Macbeth: \" , len(sm_not_repeated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnVf4ixpi_KE",
        "outputId": "32478167-a529-4ec1-c109-dda0013aa506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of repeated words in Caesar:  17788\n",
            "Number of repeated words in Hamlet:  25567\n",
            "Number of repeated words in Macbeth:  14825\n"
          ]
        }
      ],
      "source": [
        "# d)\n",
        "\n",
        "sc_repeated_words = sc_number_of_words - len(sc_not_repeated)\n",
        "sh_repeated_words = sh_number_of_words - len(sh_not_repeated)\n",
        "sm_repeated_words = sm_number_of_words - len(sm_not_repeated)\n",
        "\n",
        "print(\"Number of repeated words in Caesar: \" , sc_repeated_words)\n",
        "print(\"Number of repeated words in Hamlet: \" , sh_repeated_words)\n",
        "print(\"Number of repeated words in Macbeth: \" , sm_repeated_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM_2KMHRi_KE",
        "outputId": "e1749673-be1c-4168-ce17-f59111752c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of words per sentence in Caesar:  9.617198335644938\n",
            "Average number of words per sentence in Hamlet:  9.74436574372183\n",
            "Average number of words per sentence in Macbeth:  9.581541688515994\n"
          ]
        }
      ],
      "source": [
        "# e)\n",
        "\n",
        "sc_word_per_sent = sc_number_of_words / len(sc_sents)\n",
        "sh_word_per_sent = sh_number_of_words / len(sh_sents)\n",
        "sm_word_per_sent = sm_number_of_words / len(sm_sents)\n",
        "\n",
        "print(\"Average number of words per sentence in Caesar: \" , sc_word_per_sent)\n",
        "print(\"Average number of words per sentence in Hamlet: \" , sh_word_per_sent)\n",
        "print(\"Average number of words per sentence in Macbeth: \" , sm_word_per_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt-RUUhbi_KE"
      },
      "source": [
        "2) Em relação ao corpus “gutenberg” implemente os algoritmos para\n",
        "responder às seguintes questões:<br>\n",
        "a) Total de palavras em cada documento do corpus \"gutenberg\"<br>\n",
        "b) Quem é o maior documento do corpus?<br>\n",
        "c) Quem é o menor documento do corpus?<br>\n",
        "d) Calcular a média da quantidade sentenças por palavras do\n",
        "corpus “gutenberg”.<br>\n",
        "e) Calcule a distribuição de frequência das palavras do livro\n",
        "\"shakespeare-macbeth.txt\".<br>\n",
        "f) Calcule 5 palavras mais frequentes nesse corpus.<br>\n",
        "g) Mostre a diferença entre de palavras entre dois livros.\n",
        "(shakespeare-caesar.txt, shakespeare-hamlet.txt,)<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA7mqbtvi_KE",
        "outputId": "cbd37e11-2f35-444e-c3c1-eec1d4bcd678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "austen-emma.txt: 161600\n",
            "austen-persuasion.txt: 84121\n",
            "austen-sense.txt: 120733\n",
            "bible-kjv.txt: 791842\n",
            "blake-poems.txt: 6934\n",
            "bryant-stories.txt: 46611\n",
            "burgess-busterbrown.txt: 16327\n",
            "carroll-alice.txt: 27333\n",
            "chesterton-ball.txt: 82682\n",
            "chesterton-brown.txt: 73286\n",
            "chesterton-thursday.txt: 58724\n",
            "edgeworth-parents.txt: 170737\n",
            "melville-moby_dick.txt: 218361\n",
            "milton-paradise.txt: 80493\n",
            "shakespeare-caesar.txt: 20802\n",
            "shakespeare-hamlet.txt: 30266\n",
            "shakespeare-macbeth.txt: 18272\n",
            "whitman-leaves.txt: 126276\n"
          ]
        }
      ],
      "source": [
        "# a)\n",
        "\n",
        "gutenberg_corpus = gutenberg.fileids()\n",
        "corpus_word_counts = []\n",
        "total_words = total_sents = 0\n",
        "\n",
        "for corpus in gutenberg_corpus:\n",
        "    words = gutenberg.words(corpus)\n",
        "\n",
        "    norm_words = [word.lower() for word in words if word.isalpha()]\n",
        "\n",
        "    count_words = len(norm_words)\n",
        "    count_sents = len(gutenberg.sents(corpus))\n",
        "\n",
        "    total_words += count_words\n",
        "    total_sents += count_sents\n",
        "\n",
        "    corpus_word_counts.append((count_words, corpus))\n",
        "\n",
        "    print(corpus + \":\" , len(norm_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdRVJ5qzi_KE",
        "outputId": "c63f6aef-6be5-4809-923b-da3abb2d3241"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(791842, 'bible-kjv.txt')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# b)\n",
        "\n",
        "max(corpus_word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaE7REYNi_KF",
        "outputId": "89bdbc22-a143-4782-9e6b-d9eeda26f287"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6934, 'blake-poems.txt')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# c)\n",
        "\n",
        "min(corpus_word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxFmn8b_i_KF",
        "outputId": "f2d48799-0ef9-4ca7-b3c9-57c8095bdc4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of sentences per word:  0.04615154069495177\n"
          ]
        }
      ],
      "source": [
        "# d)\n",
        "\n",
        "sents_per_words = total_sents / total_words\n",
        "print(\"Average number of sentences per word: \" , sents_per_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yZg9t5ui_KF",
        "outputId": "0d3c3856-049a-4581-ade4-b5e07c666586"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 650),\n",
              " ('and', 546),\n",
              " ('to', 384),\n",
              " ('i', 348),\n",
              " ('of', 338),\n",
              " ('a', 241),\n",
              " ('that', 238),\n",
              " ('d', 224),\n",
              " ('you', 206),\n",
              " ('my', 203)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# e)\n",
        "\n",
        "sm_freq = FreqDist(sm_norm_words)\n",
        "sm_freq.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx_vCiFTi_KF",
        "outputId": "1fa7a831-d8dd-4d42-ac2a-8f0378707cfd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 650), ('and', 546), ('to', 384), ('i', 348), ('of', 338)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# f)\n",
        "\n",
        "sm_freq.most_common(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkbWGH9Hi_KF",
        "outputId": "de6f26e7-27ef-42c5-e07b-45af9f7a613f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in Caesar that are not in Hamlet:  ['cai', 'sold', 'vngently', 'sayd', 'cobble', 'crimson', 'dreamt', 'enclosed', 'retreat', 'wreath']\n",
            "Words in Hamlet that are not in Caesar:  ['edge', 'reueale', 'spade', 'diligence', 'entrance', 'ennactors', 'aduenturous', 'espials', 'illium', 'tedious']\n"
          ]
        }
      ],
      "source": [
        "# g)\n",
        "\n",
        "caesar_not_hamlet = list(set(sc_norm_words) - set(sh_norm_words))\n",
        "hamlet_not_caesar = list(set(sh_norm_words) - set(sc_norm_words))\n",
        "\n",
        "print(\"Words in Caesar that are not in Hamlet: \" , caesar_not_hamlet[:10])\n",
        "print(\"Words in Hamlet that are not in Caesar: \" , hamlet_not_caesar[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og4lr9jei_KF"
      },
      "source": [
        "3) Para o corpus “shakespeare-caesar.txt”, usando expressões\n",
        "regulares responda os seguintes itens:<br>\n",
        "a) Quantidades de palavras que terminam com “r”;<br>\n",
        "b) Quantidade de palavras com 5 letras;<br>\n",
        "c) Quantidade de vezes que “err” ocorre no corpus;<br>\n",
        "d) Quantidade de vezes que “are” ocorre no corpus para as\n",
        "palavras com 5 ou mais caracteres<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM8qMMMLi_KF",
        "outputId": "0cc3cdd7-1919-4561-cd6c-ef6c47d1fb1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words with r in Caesar: 1323\n"
          ]
        }
      ],
      "source": [
        "# a)\n",
        "\n",
        "count = 0\n",
        "\n",
        "def find_words_with_r(sentence):\n",
        "    pattern = r'\\w+r\\b'\n",
        "    words_found = re.findall(pattern, sentence)\n",
        "    return words_found\n",
        "\n",
        "for word in sc_norm_words:\n",
        "    if find_words_with_r(word):\n",
        "        count += 1\n",
        "\n",
        "print(\"Number of words with r in Caesar:\" , count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpMJA98ki_KF",
        "outputId": "cc00b193-75f1-4eb1-95d9-9a3d4e09af40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words with 5 letters in Caesar: 2952\n"
          ]
        }
      ],
      "source": [
        "# b)\n",
        "\n",
        "count = 0\n",
        "\n",
        "def find_words_with_5(sentence):\n",
        "    pattern = r'\\b\\w{5}\\b'\n",
        "    words_found = re.findall(pattern, sentence)\n",
        "    return words_found\n",
        "\n",
        "for word in sc_norm_words:\n",
        "    if find_words_with_5(word):\n",
        "        count += 1\n",
        "\n",
        "print(\"Number of words with 5 letters in Caesar:\" , count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFwgaBkKi_KF",
        "outputId": "adbde859-bfdc-4c33-bf3e-c7307dc309cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words with err in Caesar: 16\n"
          ]
        }
      ],
      "source": [
        "# c)\n",
        "\n",
        "count = 0\n",
        "def find_words_with_err(sentence):\n",
        "    pattern = re.compile(r'err', re.IGNORECASE)\n",
        "    words_found = pattern.findall(sentence)\n",
        "    return words_found\n",
        "\n",
        "for word in sc_norm_words:\n",
        "    if find_words_with_err(word):\n",
        "        count += 1\n",
        "\n",
        "print(\"Number of words with err in Caesar:\" , count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsqR2mgpi_KG",
        "outputId": "22223614-ab8b-4b79-854c-2346d58c240b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words with 5 letters and are in Caesar: 111\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "\n",
        "def find_words_with_are(sentence):\n",
        "    pattern = re.compile(r'are', re.IGNORECASE)\n",
        "    words_found = pattern.findall(sentence)\n",
        "    return words_found\n",
        "\n",
        "for word in sc_norm_words:\n",
        "    if find_words_with_5(word) and find_words_with_are(word):\n",
        "        count += 1\n",
        "\n",
        "print(\"Number of words with 5 letters and are in Caesar:\" , count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ3hecr9i_KG"
      },
      "source": [
        "4) Em relação do corpus “shakespeare-hamlet.txt”, faça as seguintes\n",
        "atividades:<br>\n",
        "a) Normalize o corpus (Retire os números e deixe todas as\n",
        "palavras minúsculas);<br>\n",
        "b) Aplique o lematizador em todas as palavras do corpus;<br>\n",
        "c) Aplique o tokenizador em todas as sentenças do corpus;<br>\n",
        "d) Aplique os pos tagger e responda a quantidade de adjetivos\n",
        "existem no corpus; Dica a tag de adjetivo é “JJ”<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctwkzNzNi_KG",
        "outputId": "d747b897-6f6f-4eb6-e0e1-f75fa8c9184d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'tragedie', 'of', 'hamlet', 'by', 'william', 'shakespeare', 'actus', 'primus', 'scoena']\n"
          ]
        }
      ],
      "source": [
        "# a)\n",
        "\n",
        "print(sh_norm_words[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVtQCCgUi_KG",
        "outputId": "a261102a-4724-4e83-9604-7a0bc615cd30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'tragedi', 'of', 'hamlet', 'by', 'william', 'shakespear', 'actu', 'primu', 'scoena']\n"
          ]
        }
      ],
      "source": [
        "# b)\n",
        "\n",
        "lema = WordNetLemmatizer()\n",
        "porter = PorterStemmer()\n",
        "\n",
        "new_list = []\n",
        "lemma_sh = []\n",
        "\n",
        "for word in sh_norm_words:\n",
        "    new_list.append(porter.stem(word))\n",
        "\n",
        "for word in new_list:\n",
        "  lemma_sh.append(lema.lemmatize(word))\n",
        "\n",
        "print(lemma_sh[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soIT3qpPi_KG",
        "outputId": "799907d8-b0f0-458d-88c1-544f97b36ba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'tragedi', 'of', 'hamlet', 'by', 'william', 'shakespear', 'actu', 'primu', 'scoena']\n"
          ]
        }
      ],
      "source": [
        "# c)\n",
        "\n",
        "sh_text = \" \".join(lemma_sh)\n",
        "sh_tokenize = word_tokenize(sh_text)\n",
        "print(sh_tokenize[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v-1voCei_KG",
        "outputId": "7dd51715-6192-4d22-cf15-b0aff85229f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of adjectives in Hamlet:  2874\n"
          ]
        }
      ],
      "source": [
        "# d)\n",
        "\n",
        "count = 0\n",
        "\n",
        "for word in nltk.pos_tag(sh_tokenize):\n",
        "    if word[1] == 'JJ':\n",
        "        count += 1\n",
        "\n",
        "print(\"Number of adjectives in Hamlet: \" , count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exb8pq92i_KG"
      },
      "source": [
        "5) Retire as stopwords dos seguintes textos:<br>\n",
        "a) shakespeare-caesar.txt<br>\n",
        "b) shakespeare-hamlet.txt<br>\n",
        "c) Qual é a quantidade de palavras restantes em cada texto?<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8s_O8_fi_KG",
        "outputId": "d3b44de4-7ef0-4ea2-d85a-c8b52c5dd24b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tragedie julius caesar william shakespeare actus primus scoena prima enter flauius murellus certaine\n"
          ]
        }
      ],
      "source": [
        "# a)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "sc_text = \" \".join(sc_norm_words)\n",
        "\n",
        "sc_text_without_stopwords = remove_stopwords(sc_text)\n",
        "print(sc_text_without_stopwords[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ojG7wgDi_KG",
        "outputId": "6ce3e990-3130-4dbb-bcf9-1716aac27c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tragedie hamlet william shakespeare actus primus scoena prima enter barnardo francisco two centinels\n"
          ]
        }
      ],
      "source": [
        "# b)\n",
        "\n",
        "sh_text = \" \".join(sh_norm_words)\n",
        "sh_text_without_stopwords = remove_stopwords(sh_text)\n",
        "print(sh_text_without_stopwords[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEvgLiFfi_KH",
        "outputId": "affe3cb1-e8f0-47b6-878c-e15fa7df1596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in Caesar before removing stopwords:  20802\n",
            "Number of words in Hamlet before removing stopwords:  30266\n",
            "Number of words in Caesar after removing stopwords:  68717\n",
            "Number of words in Hamlet after removing stopwords:  99483\n"
          ]
        }
      ],
      "source": [
        "# c)\n",
        "\n",
        "print(\"Number of words in Caesar before removing stopwords: \" , len(sc_norm_words))\n",
        "print(\"Number of words in Hamlet before removing stopwords: \" , len(sh_norm_words))\n",
        "\n",
        "print(\"Number of words in Caesar after removing stopwords: \" , len(sc_text_without_stopwords))\n",
        "print(\"Number of words in Hamlet after removing stopwords: \" , len(sh_text_without_stopwords))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8WMjLgMi_KH"
      },
      "source": [
        "6) Encontre as entidades nomeada presentes para o seguintes textos:\n",
        "apoloxi.txt e french-revolution.txt<br>\n",
        "a) Qual é a quantidade de entidades \"GPE\" presentes em cada\n",
        "um dos texto?<br>\n",
        "b) Qual é a quantidade de entidades \"LOCATION\" presentes\n",
        "em cada um dos texto?<br>\n",
        "c) Qual é a quantidade de entidades \"PERSON\" presentes em\n",
        "cada um dos texto?<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FfRl-0uHi_KH"
      },
      "outputs": [],
      "source": [
        "with open(\"./data/apoloxi.txt\", \"r\", encoding=\"utf-8\") as apoloxi:\n",
        "    apoloxi_text = apoloxi.read()\n",
        "with open(\"./data/french-revolution.txt\", \"r\", encoding=\"utf-8\") as french:\n",
        "    french_text = french.read()\n",
        "\n",
        "doc_apoloxi_text = nlp(apoloxi_text)\n",
        "doc_french_text = nlp(french_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfjctS5ni_KH",
        "outputId": "86acf64b-ae04-4db4-e4f8-884d8ed2bc90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPE in Apoloxi:  3\n",
            "Number of GPE in French Revolution:  34\n"
          ]
        }
      ],
      "source": [
        "# a)\n",
        "\n",
        "count_apoloxi = 0\n",
        "count_french = 0\n",
        "\n",
        "for entity in doc_apoloxi_text.ents:\n",
        "    if entity.label_ == \"GPE\":\n",
        "        count_apoloxi += 1\n",
        "\n",
        "for entity in doc_french_text.ents:\n",
        "    if entity.label_ == \"GPE\":\n",
        "        count_french += 1\n",
        "\n",
        "print(\"Number of GPE in Apoloxi: \" , count_apoloxi)\n",
        "print(\"Number of GPE in French Revolution: \" , count_french)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuF3f1d4i_KH",
        "outputId": "1f804853-8dab-4725-9248-e6690b57f03c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of LOC in Apoloxi:  7\n",
            "Number of LOC in French Revolution:  10\n"
          ]
        }
      ],
      "source": [
        "# b)\n",
        "\n",
        "count_apoloxi = 0\n",
        "count_french = 0\n",
        "\n",
        "for entity in doc_apoloxi_text.ents:\n",
        "    if entity.label_ == \"LOC\":\n",
        "        count_apoloxi += 1\n",
        "\n",
        "for entity in doc_french_text.ents:\n",
        "    if entity.label_ == \"LOC\":\n",
        "        count_french += 1\n",
        "\n",
        "print(\"Number of LOC in Apoloxi: \" , count_apoloxi)\n",
        "print(\"Number of LOC in French Revolution: \" , count_french)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXLX1YFii_KH",
        "outputId": "0c726f24-2dd0-4628-e6b3-d6c963f2afd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of PERSON in Apoloxi:  11\n",
            "Number of PERSON in French Revolution:  2\n"
          ]
        }
      ],
      "source": [
        "# c)\n",
        "\n",
        "count_apoloxi = 0\n",
        "count_french = 0\n",
        "\n",
        "for entity in doc_apoloxi_text.ents:\n",
        "    if entity.label_ == \"PERSON\":\n",
        "        count_apoloxi += 1\n",
        "\n",
        "for entity in doc_french_text.ents:\n",
        "    if entity.label_ == \"PERSON\":\n",
        "        count_french += 1\n",
        "\n",
        "print(\"Number of PERSON in Apoloxi: \" , count_apoloxi)\n",
        "print(\"Number of PERSON in French Revolution: \" , count_french)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7OmvVHSi_KH"
      },
      "source": [
        "7) Utililizando as técnicas aprendidas sobre análise de sentimentos<br>\n",
        "defina a polaridade do arquivo ‘reviews”. Utiliza os arquivos<br>\n",
        "positive_words.txt e negative_words.txt presentes na pasta.<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NC87N1gMi_KH"
      },
      "outputs": [],
      "source": [
        "negative = []\n",
        "with open(\"./data/negative_words.csv\", \"r\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        negative.append(row)\n",
        "\n",
        "positive = []\n",
        "with open(\"./data/positive_words.csv\", \"r\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        positive.append(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uYbJ71lhi_KH"
      },
      "outputs": [],
      "source": [
        "def sentiment(text):\n",
        "    temp = []\n",
        "    text_sent = nltk.sent_tokenize(text)\n",
        "    for sentence in text_sent:\n",
        "        n_count = 0\n",
        "        p_count = 0\n",
        "        sent_words = nltk.word_tokenize(sentence)\n",
        "        for word in sent_words:\n",
        "            for item in positive:\n",
        "                if(word == item[0]):\n",
        "                    p_count +=1\n",
        "            for item in negative:\n",
        "                if(word == item[0]):\n",
        "                    n_count +=1\n",
        "        if(p_count > 0 and n_count == 0):\n",
        "            temp.append(1)\n",
        "        elif(n_count%2 > 0):\n",
        "            temp.append(-1)\n",
        "        elif(n_count%2 ==0 and n_count > 0):\n",
        "            temp.append(1)\n",
        "        else:\n",
        "            temp.append(0)\n",
        "    return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hQIibvVi_KK",
        "outputId": "d98ef7b8-8821-4666-dcc2-f4eaf54ec90f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "0.6\n",
            "This camera is perfect for an enthusiastic amateur photographer. The pictures are razor-sharp, even \n",
            "\n",
            "\n",
            "0.875\n",
            "I got my camera three days back, and although i had some experience with digital cameras prior to pu\n",
            "\n",
            "\n",
            "0.6666666666666666\n",
            "I love photography. I had an older camera that was simply a point and shoot camera. I needed somethi\n",
            "\n",
            "\n",
            "0.18181818181818182\n",
            "I bought coolpix 4300 two months after i had bought canon powershot s400. It was not easy sharing on\n",
            "\n",
            "\n",
            "0.2857142857142857\n",
            "The other reviewers have clearly pointed all the good things about this camera, which i do agree. Bu\n",
            "\n",
            "\n",
            "-0.2857142857142857\n",
            "Within a year, there are problems with my menu dial knob. It became stuck which makes it almost impo\n",
            "\n",
            "\n",
            "0.0\n",
            "Got a \"system error\" problem 30 days after purchase. Made the camera totally inoperable. Also, the l\n",
            "\n",
            "\n",
            "0.45454545454545453\n",
            "I am an amateur photographer and here is a piece of advise to all the folks who are thinking about m\n",
            "\n",
            "\n",
            "2.778030303030303\n"
          ]
        }
      ],
      "source": [
        "comments = []\n",
        "polarytie = 0\n",
        "\n",
        "with open(\"./data/reviews.csv\", \"r\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        comments.append(row)\n",
        "\n",
        "for review in comments:\n",
        "    print(\"\\n\")\n",
        "    print(np.average(sentiment(str(review))))\n",
        "    polarytie += np.average(sentiment(str(review)))\n",
        "    print(review[0][:100])\n",
        "\n",
        "print(\"\\n\")\n",
        "print(polarytie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vZnA_8xi_KK"
      },
      "source": [
        "8) Utililizando as técnicas aprendidas sobre Term Frequency,\n",
        "Inverse Document Frequency, liste as 5 palavras mais relevantes de\n",
        "cada texto contido no corpus Gutemberg. A sua analise deve ser em\n",
        "cima dos seguintes textos : 'austen-emma.txt', 'bible-kjv.txt',\n",
        "'carroll-alice.txt', 'melville-moby_dick.txt', 'shakespeare-caesar.txt' e\n",
        "'shakespeare-hamlet.txt'. Lembre-se que antes de aplicar o as técnicas\n",
        "de TF IDF o texto deve está normalizado, com as stopwords retiradas e\n",
        "lematizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vRleBHJi_KK",
        "outputId": "99ef7a55-a599-4b32-f713-ecd9e59ddfb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "austen-emma.txt\n",
            "emma, harriet, weston, elton, knightley\n",
            "\n",
            "\n",
            "bible-kjv.txt\n",
            "unto, shall, lord, thou, god\n",
            "\n",
            "\n",
            "carroll-alice.txt\n",
            "alice, said, hatter, little, gryphon\n",
            "\n",
            "\n",
            "melville-moby_dick.txt\n",
            "whale, ahab, one, ship, boat\n",
            "\n",
            "\n",
            "shakespeare-caesar.txt\n",
            "bru, caesar, brutus, haue, cassi\n",
            "\n",
            "\n",
            "shakespeare-hamlet.txt\n",
            "ham, haue, lord, hamlet, king\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "texts = [\n",
        "    'austen-emma.txt',\n",
        "    'bible-kjv.txt',\n",
        "    'carroll-alice.txt',\n",
        "    'melville-moby_dick.txt',\n",
        "    'shakespeare-caesar.txt',\n",
        "    'shakespeare-hamlet.txt'\n",
        "]\n",
        "\n",
        "corpus = [gutenberg.raw(text) for text in texts]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "corpus_preprocessed = [preprocess_text(text) for text in corpus]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus_preprocessed)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "for i, text in enumerate(texts):\n",
        "    print(text)\n",
        "\n",
        "    relevant_word_indices = tfidf_matrix[i].indices[np.argsort(tfidf_matrix[i].data)[::-1][:5]]\n",
        "\n",
        "    relevant_words = [feature_names[idx] for idx in relevant_word_indices]\n",
        "    print(\", \".join(relevant_words))\n",
        "    print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}